{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time Series - Maker workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick round table\n",
    "\n",
    "Presentation & expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "- **Time series data is data that is collected at different points in time.** This is opposed to cross-sectional data which observes individuals, companies, etc. at a single point in time.\n",
    "\n",
    "\n",
    "- If you previously followed the *Maker workshop dedicated to Machine Learning*, you've already worked with cross-sectional data, but not time series.\n",
    "\n",
    "\n",
    "- Time series can be found in a wide variety of domains: in economics, social sciences, medicine, but also ( and obviously) in physical sciences and engineering. As a result, **we deal with them a lot at Total!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. Today's challenge\n",
    "2. Today's Data Science environment checklist\n",
    "3. Exploring the data \n",
    "    - Types, indexes and unique values\n",
    "    - Distributions\n",
    "    - Correlations\n",
    "4. Dealing with missing values\n",
    "5. Resampling techniques\n",
    "6. Time series visualization\n",
    "7. Anomalies detection techniques\n",
    "8. Forecasting\n",
    "8. Open discussion / work session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today's Challenge\n",
    "\n",
    "**Predict the air temperature in 2017 based on weather data from 2009 to 2016.**\n",
    "\n",
    "- Features available:\n",
    "    - Air temperature\n",
    "    - Atmospheric pressure\n",
    "    - Humidity\n",
    "    - Wind direction\n",
    "    - Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today's Data Science environment checklist\n",
    "\n",
    "- A Jupyter notebook\n",
    "- The data folder (the one that we sent)\n",
    "- The following libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! make -f ../setup/Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "# Optional\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment this if you don't have the data\n",
    "# !wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
    "# !unzip jena_climate_2009_2016.csv.zip\n",
    "\n",
    "# Used for data preparation\n",
    "\n",
    "#raw_data_bis = pd.read_csv('../data/jena_climate_2009_2016.csv')\n",
    "#raw_data_bis['open_st'] = 1.0\n",
    "#\n",
    "#df = raw_data_bis[['VPmax (mbar)', 'VPact (mbar)']].copy()\n",
    "#import random\n",
    "#ix = [(row, col) for row in range(df.shape[0]) for col in range(df.shape[1])]\n",
    "#for row, col in random.sample(ix, int(round(.01*len(ix)))):\n",
    "#    df.iat[row, col] = np.nan\n",
    "#\n",
    "#raw_data_bis[['VPmax (mbar)', 'VPact (mbar)']] = df.copy()\n",
    "#\n",
    "#raw_data_bis.to_csv('../data/jena_climate_2009_2016.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading the raw data\n",
    "\n",
    "- `head -n 10` is a useful shell command to give a look at a file's header (the first 10 lines in this case)\n",
    "- In a Jupyter notebook, we can use the symbol `!` to run shell commands\n",
    "\n",
    "_What are the useful details that you can see thanks to this command?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!head -n 10 ../data/jena_climate_2009_2016.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we have a better idea of the file's format, we can implement our reading function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../data/jena_climate_2009_2016.csv', sep=',')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data types\n",
    "\n",
    "- Checking for data types is useful to make sure that types were properly inferred when reading the raw CSV file\n",
    "- If you've already explored the data, you can specify the undetected types in the `pandas.read_csv` function\n",
    "- Tip: Casting to smaller float types can help you tremendly reduce the size of a dataset\n",
    "\n",
    "_Comment on the following dtypes. Do you think the proper types were inferred?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "raw_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Indexing\n",
    "\n",
    "- When dealing with time series, we'll see that it can be useful to make the most out of pandas' `DatetimeIndex`, i.e. to set a `Datetime` column as index of the dataframe.\n",
    "\n",
    "_Let's verify if the Datetime type was correctly inferred from the CSV file._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "type(raw_data['Date Time'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Checking duplicated rows\n",
    "\n",
    "- Before to continue the data manipulation, we should check for potential duplicated rows in the data that we want to get rid of.\n",
    "\n",
    "_What is the percentage of duplicated rows among the complete dataset?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "percentage = raw_data[raw_data.duplicated(subset=['Date Time'])].shape[0] / raw_data.shape[0] * 100\n",
    "print(f'Among the complete data, {round(percentage * 100, 2)}% are duplicated rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "raw_data.drop_duplicates(subset='Date Time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unique values\n",
    "\n",
    "- Checking for unique values will give you information on your variables' granularity:\n",
    "    - A small number of unique values can indicate the presence of a category\n",
    "    - A single unique value may indicate that a variable is never changing, even out of your sample\n",
    "   \n",
    "_Do you notice any of these two cases in your dataset?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for col in raw_data.columns: \n",
    "    print(col, ' '*(20-len(col))+'----->', len(raw_data[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distributions\n",
    "\n",
    "- With the `seaborn` library, we can easily plot the distributions and the relationship between each pair of sensors\n",
    "\n",
    "_Let's give a look at the following graph: from your functional knowledge of the sensors, can you identify normal or abnormal patterns?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_COLUMNS = ['p (mbar)', 'T (degC)', 'H2OC (mmol/mol)', 'sh (g/kg)', 'wd (deg)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(raw_data[SELECTED_COLUMNS])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlations\n",
    "\n",
    "- Correlation analysis is a statistical method used to **evaluate the strength of relationship between two quantitative variables**. \n",
    "\n",
    "\n",
    "- A high correlation means that two or more variables have a strong relationship with each other.\n",
    "- A weak correlation means that the variables are hardly related.\n",
    "\n",
    "_Let's continue our analysis by plotting the correlation matrix. Do you notice anything?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def print_correlation(df):\n",
    "    corr = df.corr()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    ax = sns.heatmap(corr, vmin=-1, vmax=1, center=0,\n",
    "                     cmap=sns.diverging_palette(20, 220, n=200),\n",
    "                     square=True, annot=True)\n",
    "    \n",
    "    ax.set_xticklabels(ax.get_xticklabels(),\n",
    "                       rotation=45,\n",
    "                       horizontalalignment='right')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print_correlation(raw_data[SELECTED_COLUMNS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use our own custom function to get a stylized correlation matrix. However, you could simply use the pandas method _your_dataframe_name.corr()_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identification of missing values\n",
    "\n",
    "_Let's identify missing values in our dataset._\n",
    "\n",
    "Tip: Knowing that our variables are floats, the missing values will appear as `NaN` (= Not A Number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "raw_data[raw_data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Assess the time series delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = raw_data.copy()\n",
    "sorted_data['Date Time'] = pd.to_datetime(sorted_data['Date Time'], format='%d.%m.%Y %H:%M:%S')\n",
    "sorted_data =  sorted_data.sort_values('Date Time')\n",
    "\n",
    "dt_index_delta = sorted_data['Date Time'].diff()\n",
    "dt_index_delta.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Lucky for us, we have is a nice `resample()` method for pandas dataframes that have a DatetimeIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create a DatetimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "reindexed_data = sorted_data.copy()\n",
    "reindexed_data.set_index('Date Time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to better illustrate the concept of resampling, let's create a fake sinusoidal time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def create_fake_ts(length=10000):\n",
    "    raw_values = np.sin(np.linspace(1, length, length)*2*np.pi/(24*60))\n",
    "    date = pd.date_range(start='2017-01-01', periods=len(raw_values), freq='min')\n",
    "    \n",
    "    sampled = [i for i in range(len(raw_values)//2, len(raw_values), 8*60)]\n",
    "    sampled_date = date[sampled]\n",
    "\n",
    "    raw_less_values = raw_values[sampled]\n",
    "\n",
    "    raw_df = pd.DataFrame(raw_values[:sampled[0]])\n",
    "    raw_df.index = date[:sampled[0]]\n",
    "\n",
    "    raw_less_values_df = pd.DataFrame(raw_less_values)\n",
    "    raw_less_values_df.index = sampled_date\n",
    "\n",
    "    raw_df = raw_df.append(raw_less_values_df)\n",
    "    \n",
    "    raw_df.index.name = 'date'\n",
    "    raw_df.columns = ['values']\n",
    "\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ts_to_resample = create_fake_ts(length=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply the same analysis of the time differences as in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ts_to_resample.reset_index(inplace=True)\n",
    "\n",
    "ts_to_resample['delta'] = ts_to_resample['date'].diff()\n",
    "\n",
    "ts_to_resample.set_index('date', inplace=True)\n",
    "\n",
    "ts_to_resample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.scatter(x=ts_to_resample.index, y=ts_to_resample['values'])\n",
    "\n",
    "plt.title('Time series to resample')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "(ts_to_resample['delta'].dt.total_seconds() / (3600 * 24)).plot()\n",
    "\n",
    "plt.title('Differences between timestamps')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example data, starting on 2017-01-04, we no longer receive 1 data point every minute but instead 1 data point every 8h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our case, we have 2 different ways to go to obtain a time series with equally-spaced data points:\n",
    "1. Remove some data points from the first part of the time series in order to get 8h-spaced data points similar to the second part of the time series: this technique is known as **undersampling**.\n",
    "2. Add some data points in the second part of the time series in order to get 1min-spaced data points similar to the first part of the time series: this technique is known as **oversampling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Forward-fill method\n",
    "\n",
    "Even though it depends on the problem at hand, a way to go can be to use the frequency the most represented in order not to add/remove too many data points.\n",
    "\n",
    "When oversampling your time series, i.e. creating new data points, you'll need to make a decision regarding which values to assign to these new points.\n",
    "\n",
    "When resampling time series, a common risk is to introduce **data leakage** by adding data from the future to the past, i.e. data that would not have been available at the time.\n",
    "\n",
    "For example, if you decide to fill a missing data point at time t with the next available values (this is known as **backward filling**), how would you have been able to do that at time t, knowing that these future values were not available at the time?\n",
    "\n",
    "You should always ask yourself this question when manipulating time series, especially when adding data points and creating new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../setup/images/ffill.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As presented on this diagram, you should always use **forward filling** as a resampling method when oversampling. Backward filling would bring in unavailable values from the future and introduce a data leak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ts_to_resample_min = ts_to_resample['values'].resample('T').ffill()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.plot(ts_to_resample_min, color='orange')\n",
    "plt.scatter(x=ts_to_resample.index, y=ts_to_resample['values'])\n",
    "\n",
    "plt.title('Resampling with the forward fill method')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Other methods - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ts_to_resample_min = ts_to_resample['values'].resample('T').interpolate(method='linear')\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.plot(ts_to_resample_min, color='orange')\n",
    "plt.scatter(x=ts_to_resample.index, y=ts_to_resample['values'])\n",
    "\n",
    "plt.title('Resampling with the linear method')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Other methods - Nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ts_to_resample_min = ts_to_resample['values'].resample('T').interpolate(method='nearest')\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.plot(ts_to_resample_min, color='orange')\n",
    "plt.scatter(x=ts_to_resample.index, y=ts_to_resample['values'])\n",
    "\n",
    "plt.title('Resampling with the \"nearest\" method')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained earlier, if you look closely at the different graphs, you'll realize that **the forward fill method is the only method presented which doesn't introduce any data leak.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mix of undersampling and oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_to_resample_min = ts_to_resample['values'].resample('4H').ffill()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.plot(ts_to_resample_min, color='orange')\n",
    "plt.scatter(x=ts_to_resample.index, y=ts_to_resample['values'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we know everything we need to resample our data on a 10-min basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = reindexed_data.resample('10min').ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time series visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Global view & analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "\n",
    "for col in clean_data.columns:\n",
    "    plt.plot(clean_data[col], label=col)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Sensor values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sensor-level view & analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for col in clean_data.columns:\n",
    "    \n",
    "    plt.figure(figsize=(20, 7))\n",
    "    plt.plot(clean_data[col])\n",
    "    plt.title(col)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anomalies detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- An anomaly is an outlier data point, which does not follow the collective common pattern of the majority of the data points and hence can be easily separated or distinguished from the rest of the data.\n",
    "\n",
    "- In our case, we can try to identify abnormal temperatures over the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fix threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TAG_NAME = 'T (degC)'\n",
    "\n",
    "plt.figure(figsize=(20, 7))\n",
    "\n",
    "plt.plot(clean_data[TAG_NAME])\n",
    "plt.title(TAG_NAME)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "_What could be a relevant threshold to apply to this specific sensor ?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, consider that we apply the following thresholds (upper/lower) for the specified sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SENSORS_THRESHOLDS = {TAG_NAME:[-15, 34]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's backtest our **fix threshold** strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting_df = clean_data.copy()\n",
    "\n",
    "for col in SENSORS_THRESHOLDS.keys():\n",
    "    upper_alert = (backtesting_df[col] > SENSORS_THRESHOLDS[col][1])\n",
    "    lower_alert = (backtesting_df[col] < SENSORS_THRESHOLDS[col][0])\n",
    "    \n",
    "    backtesting_df[f'is_alert_{col}'] = (upper_alert | lower_alert).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "\n",
    "plt.plot(backtesting_df[f'is_alert_{TAG_NAME}'], color='red')\n",
    "plt.title(f'Fix threshold - Alerting state on sensor {TAG_NAME}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Statistical profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a statistical profile of the data can be the fastest and the most useful approach, and it still offers a **clear and explainable outcome**.\n",
    "\n",
    "- In the case of statistical profiling, **we use the mean, median, standard deviations and/or quantiles to come up with upper and lower bounds** to detect anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "sns.boxplot(clean_data[TAG_NAME])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, consider that we use the 1st and 99th quantiles for the specified sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTILE_PARAM = 0.99\n",
    "\n",
    "upper_quantile = clean_data[TAG_NAME].quantile(QUANTILE_PARAM)\n",
    "lower_quantile = clean_data[TAG_NAME].quantile(1-QUANTILE_PARAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's backtest our **statistical profiling** strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting_df = clean_data.copy()\n",
    "\n",
    "for col in SENSORS_THRESHOLDS.keys():\n",
    "    upper_alert = (backtesting_df[col] > upper_quantile)\n",
    "    lower_alert = (backtesting_df[col] < lower_quantile)\n",
    "    \n",
    "    backtesting_df[f'is_alert_{col}'] = (upper_alert | lower_alert).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "\n",
    "plt.plot(backtesting_df[f'is_alert_{TAG_NAME}'], color='red')\n",
    "plt.title(f'Statistical profiling - Alerting state on sensor {TAG_NAME}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A word of caution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "One needs to be careful when predicting the future:\n",
    "\n",
    "- _\"Stocks have reached what looks like a permanently high plateau.\"_ - Irving Fischer, Professor of Economics, Yale University, 1929\n",
    "    - True or False?\n",
    "\n",
    "- _\"Computers in the future weigh no more than 1.5 tons.\"_ - Popular Mechanics, forecasting the relentless march of science, 1949\n",
    "    - True or False?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Introduction to Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Open-sourced by Facebook's core data science team a few years ago, Prophet is based on time series decomposition but has the ability to model different seasonalities as well as the effect of holidays and special events.\n",
    "\n",
    "- On [Prophet Github page](https://github.com/facebook/prophet), we find the following description:\n",
    "\n",
    "_\"Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\"_\n",
    "\n",
    "- In this section, we'll try to assess how Prophet performs to predict the future value of the temperature (the T (degC) sensor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The input to Prophet is always a DataFrame with 2 columns: `ds` and `y`:\n",
    "- The `ds` (datestamp) column should be of a format expected by pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. \n",
    "- The `y` column must be numeric, and represents the measurement we wish to forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TAG_NAME = 'T (degC)'\n",
    "\n",
    "prophet_df = clean_data.resample('1d').ffill()\n",
    "prophet_df = prophet_df[[TAG_NAME]].reset_index()\n",
    "prophet_df = prophet_df.rename(columns={'Date Time':'ds', TAG_NAME:'y'})\n",
    "\n",
    "prophet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Prophet follows the sklearn model API. We create an instance of the `Prophet `class and then call its `fit` and `predict` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Prophet()\n",
    "model.fit(prophet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we have a model, we can make predictions on a DataFrame with a column `ds` containing the dates for which a prediction is to be made. \n",
    "\n",
    "You can get a suitable DataFrame that extends into the future a specified number of days using the helper method `Prophet.make_future_dataframe` (by default, it will also include the dates from the history)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "future = model.make_future_dataframe(periods=365)\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we can apply the `predict` method to this DataFrame: it will assign each row a predicted value which it names `yhat`. If you pass in historical dates, it will provide an in-sample fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can lot the forecast by calling the `Prophet.plot` method and passing in our forecast DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig1 = model.plot(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you want to see the forecast components, you can use the `Prophet.plot_components` method. \n",
    "\n",
    "By default you’ll see the trend, yearly seasonality, and weekly seasonality of the time series. If you include holidays, you’ll see those here, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = model.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thank you!\n",
    "### Any feedback? Return on time invested?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
