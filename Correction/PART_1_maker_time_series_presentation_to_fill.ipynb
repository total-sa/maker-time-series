{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time Series - Maker workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick round table\n",
    "\n",
    "Presentation & expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "- **Time series data is data that is collected at different points in time.** This is opposed to cross-sectional data which observes individuals, companies, etc. at a single point in time.\n",
    "\n",
    "\n",
    "- If you previously followed the *Maker workshop dedicated to Machine Learning*, you've already worked with cross-sectional data, but not time series.\n",
    "\n",
    "\n",
    "- Time series can be found in a wide variety of domains: in economics, social sciences, medicine, but also ( and obviously) in physical sciences and engineering. As a result, **we deal with them a lot at Total!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. Today's challenge\n",
    "2. Today's Data Science environment checklist\n",
    "3. Exploring the data \n",
    "    - Types, indexes and unique values\n",
    "    - Distributions\n",
    "    - Correlations\n",
    "4. Dealing with missing values\n",
    "5. Resampling techniques\n",
    "6. Time series visualization\n",
    "7. Anomalies detection techniques\n",
    "8. Forecasting\n",
    "8. Open discussion / work session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today's Challenge\n",
    "\n",
    "**Predict the air temperature in 2017 based on weather data from 2009 to 2016.**\n",
    "\n",
    "- Features available:\n",
    "    - Air temperature\n",
    "    - Atmospheric pressure\n",
    "    - Humidity\n",
    "    - Wind direction\n",
    "    - Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today's Data Science environment checklist\n",
    "\n",
    "- A Jupyter notebook\n",
    "- The data folder (the one that we sent)\n",
    "- The following libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! make -f ../setup/Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "# Optional\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment this if you don't have the data\n",
    "# !wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
    "# !unzip jena_climate_2009_2016.csv.zip\n",
    "\n",
    "# Used for data preparation\n",
    "\n",
    "#raw_data_bis = pd.read_csv('../data/jena_climate_2009_2016.csv')\n",
    "#raw_data_bis['open_st'] = 1.0\n",
    "#\n",
    "#df = raw_data_bis[['VPmax (mbar)', 'VPact (mbar)']].copy()\n",
    "#import random\n",
    "#ix = [(row, col) for row in range(df.shape[0]) for col in range(df.shape[1])]\n",
    "#for row, col in random.sample(ix, int(round(.01*len(ix)))):\n",
    "#    df.iat[row, col] = np.nan\n",
    "#\n",
    "#raw_data_bis[['VPmax (mbar)', 'VPact (mbar)']] = df.copy()\n",
    "#\n",
    "#raw_data_bis.to_csv('../data/jena_climate_2009_2016.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading the raw data\n",
    "\n",
    "- `head -n 10` is a useful shell command to give a look at a file's header (the first 10 lines in this case)\n",
    "- In a Jupyter notebook, we can use the symbol `!` to run shell commands\n",
    "\n",
    "_What are the useful details that you can see thanks to this command?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Time,p (mbar),T (degC),Tpot (K),Tdew (degC),rh (%),VPmax (mbar),VPact (mbar),VPdef (mbar),sh (g/kg),H2OC (mmol/mol),rho (g/m**3),wv (m/s),max. wv (m/s),wd (deg),open_st\r\n",
      "01.01.2009 00:10:00,996.52,-8.02,265.4,-8.9,93.3,3.33,3.11,0.22,1.94,3.12,1307.75,1.03,1.75,152.3,1.0\r\n",
      "01.01.2009 00:20:00,996.57,-8.41,265.01,-9.28,93.4,3.23,3.02,0.21,1.89,3.03,1309.8,0.72,1.5,136.1,1.0\r\n",
      "01.01.2009 00:30:00,996.53,-8.51,264.91,-9.31,93.9,3.21,3.01,0.2,1.88,3.02,1310.24,0.19,0.63,171.6,1.0\r\n",
      "01.01.2009 00:40:00,996.51,-8.31,265.12,-9.07,94.2,3.26,3.07,0.19,1.92,3.08,1309.19,0.34,0.5,198.0,1.0\r\n",
      "01.01.2009 00:50:00,996.51,-8.27,265.15,-9.04,94.1,3.27,3.08,0.19,1.92,3.09,1309.0,0.32,0.63,214.3,1.0\r\n",
      "01.01.2009 01:00:00,996.5,-8.05,265.38,-8.78,94.4,3.33,3.14,0.19,1.96,3.15,1307.86,0.21,0.63,192.7,1.0\r\n",
      "01.01.2009 01:10:00,996.5,-7.62,265.81,-8.3,94.8,3.44,3.26,0.18,2.04,3.27,1305.68,0.18,0.63,166.5,1.0\r\n",
      "01.01.2009 01:20:00,996.5,-7.62,265.81,-8.36,94.4,3.44,3.25,0.19,2.03,3.26,1305.69,0.19,0.5,118.6,1.0\r\n",
      "01.01.2009 01:30:00,996.5,-7.91,265.52,-8.73,93.8,3.36,3.15,0.21,1.97,3.16,1307.17,0.28,0.75,188.5,1.0\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 ../data/jena_climate_2009_2016.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we have a better idea of the file's format, we can implement our reading function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../data/jena_climate_2009_2016.csv', sep=\"CODE HERE\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data types\n",
    "\n",
    "- Checking for data types is useful to make sure that types were properly inferred when reading the raw CSV file\n",
    "- If you've already explored the data, you can specify the undetected types in the `pandas.read_csv` function\n",
    "- Tip: Casting to smaller float types can help you tremendly reduce the size of a dataset\n",
    "\n",
    "_Comment on the following dtypes. Do you think the proper types were inferred?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "raw_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Indexing\n",
    "\n",
    "- When dealing with time series, we'll see that it can be useful to make the most out of pandas' `DatetimeIndex`, i.e. to set a `Datetime` column as index of the dataframe.\n",
    "\n",
    "_Let's verify if the Datetime type was correctly inferred from the CSV file._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "type(raw_data['Date Time'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Checking duplicated rows\n",
    "\n",
    "- Before to continue the data manipulation, we should check for potential duplicated rows in the data that we want to get rid of.\n",
    "\n",
    "_What is the percentage of duplicated rows among the complete dataset?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "percentage = raw_data[raw_data.duplicated(subset=[\"CODE HERE\"])].shape[0] / raw_data.shape[0] * 100\n",
    "print(f'Among the complete data, {round(percentage * 100, 2)}% are duplicated rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "raw_data.drop_duplicates(subset='Date Time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unique values\n",
    "\n",
    "- Checking for unique values will give you information on your variables' granularity:\n",
    "    - A small number of unique values can indicate the presence of a category\n",
    "    - A single unique value may indicate that a variable is never changing, even out of your sample\n",
    "   \n",
    "_Do you notice any of these two cases in your dataset?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for col in raw_data.columns: \n",
    "    print(col, ' '*(20-len(col))+'----->', len(raw_data[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distributions\n",
    "\n",
    "- With the `seaborn` library, we can easily plot the distributions and the relationship between each pair of sensors\n",
    "\n",
    "_Let's give a look at the following graph: from your functional knowledge of the sensors, can you identify normal or abnormal patterns?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_COLUMNS = ['p (mbar)', 'T (degC)', 'H2OC (mmol/mol)', 'sh (g/kg)', 'wd (deg)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.\"CODE HERE\"(raw_data[SELECTED_COLUMNS])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlations\n",
    "\n",
    "- Correlation analysis is a statistical method used to **evaluate the strength of relationship between two quantitative variables**. \n",
    "\n",
    "\n",
    "- A high correlation means that two or more variables have a strong relationship with each other.\n",
    "- A weak correlation means that the variables are hardly related.\n",
    "\n",
    "_Let's continue our analysis by plotting the correlation matrix. Do you notice anything?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def print_correlation(df):\n",
    "    corr = df.corr()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    ax = sns.heatmap(corr, vmin=-1, vmax=1, center=0,\n",
    "                     cmap=sns.diverging_palette(20, 220, n=200),\n",
    "                     square=True, annot=True)\n",
    "    \n",
    "    ax.set_xticklabels(ax.get_xticklabels(),\n",
    "                       rotation=45,\n",
    "                       horizontalalignment='right')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print_correlation(raw_data[SELECTED_COLUMNS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use our own custom function to get a stylized correlation matrix. However, you could simply use the pandas method _your_dataframe_name.corr()_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.to_csv('../data/jena_climate_2009_2016_part_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## See you on Part 2 ;)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
